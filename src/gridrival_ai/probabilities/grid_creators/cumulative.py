"""
Cumulative market grid creator for GridRival AI.

This module implements a grid creator that uses cumulative market odds
(win, top-3, top-6, etc.) to create more accurate position distributions.
"""

import warnings
from typing import Dict, List, Optional, Union

import numpy as np

from gridrival_ai.probabilities.distributions import (
    PositionDistribution,
    RaceDistribution,
    SessionDistribution,
)
from gridrival_ai.probabilities.grid_creators.base import GridCreator
from gridrival_ai.probabilities.normalizers import get_grid_normalizer
from gridrival_ai.probabilities.odds_structure import OddsStructure


class CumulativeGridCreator(GridCreator):
    """
    Grid creator using cumulative market odds.

    This class creates position distributions from cumulative market odds
    (win, top-3, top-6, etc.), offering more accurate position probabilities
    compared to using win odds alone.

    Parameters
    ----------
    max_position : int, optional
        Maximum finishing position to consider, by default 20
    baseline_method : str, optional
        Method for baseline weights within segments, by default "exponential"
    baseline_params : Dict, optional
        Parameters for the baseline method, by default None

    Notes
    -----
    The algorithm works by:
    1. Removing bookmaker margins from implied probabilities
    2. Segmenting positions based on cumulative markets
    3. Using baseline weights to distribute within segments
    4. Scaling weights to match the cumulative market constraints
    5. Enforcing column constraints while preserving win probabilities
    """

    def __init__(
        self,
        max_position: int = 20,
        baseline_method: str = "exponential",
        baseline_params: Optional[Dict] = None,
        **kwargs,
    ) -> None:
        """
        Initialize the cumulative grid creator.

        Parameters
        ----------
        max_position : int, optional
            Maximum finishing position to consider, by default 20
        baseline_method : str, optional
            Method for baseline weights within segments, by default "exponential"
        baseline_params : Dict, optional
            Parameters for the baseline method, by default None
        **kwargs
            Additional parameters for the parent GridCreator
        """
        super().__init__(**kwargs)
        self.max_position = max_position
        self.baseline_method = baseline_method
        self.baseline_params = baseline_params or {}

    def create_session_distribution(
        self,
        odds_input: Union[OddsStructure, Dict],
        session_type: str = "race",
        **kwargs,
    ) -> SessionDistribution:
        """
        Create a session distribution from cumulative market odds.

        Parameters
        ----------
        odds_input : Union[OddsStructure, Dict]
            OddsStructure instance or raw odds dictionary
        session_type : str, optional
            Session type to use for odds, by default "race"
        **kwargs
            Additional parameters for normalization:
            - method : str, optional
                Method to use for normalization, by default "sinkhorn"
            - Additional parameters for the chosen normalizer

        Returns
        -------
        SessionDistribution
            Session distribution for all drivers
        """
        # Ensure input is an OddsStructure
        odds_structure = self._ensure_odds_structure(odds_input)

        # Check if we have multiple markets or just win odds
        session_odds = odds_structure.get_session_odds(session_type)
        position_thresholds = list(session_odds.keys())

        # If we only have win odds, fall back to Harville method
        if len(position_thresholds) == 1 and position_thresholds[0] == 1:
            # Get win odds and driver IDs
            win_odds = session_odds[1]
            odds_list = list(win_odds.values())
            driver_ids = list(win_odds.keys())

            # Convert to probabilities
            win_probs = self.odds_converter.convert(odds_list)

            position_distributions = self._harville_fallback(win_probs, driver_ids)
        else:
            # Process cumulative markets
            cumulative_probs = {}
            for threshold in position_thresholds:
                driver_odds = session_odds[threshold]
                # Convert odds to probabilities
                odds_list = list(driver_odds.values())
                driver_ids = list(driver_odds.keys())

                # Convert to probabilities with appropriate target sum
                # Target sum for threshold n should be n (e.g., top-3 sums to 3.0)
                probs_array = self.odds_converter.convert(
                    odds_list, target_sum=float(threshold)
                )

                # Create probability dictionary
                cumulative_probs[threshold] = {
                    driver_id: prob for driver_id, prob in zip(driver_ids, probs_array)
                }

            # Get all driver IDs across all thresholds
            all_driver_ids = set()
            for threshold_probs in cumulative_probs.values():
                all_driver_ids.update(threshold_probs.keys())

            position_distributions = self._convert_cumulative_to_position_distributions(
                cumulative_probs, list(all_driver_ids)
            )

        # Create and return session distribution
        return SessionDistribution(
            position_distributions, session_type=session_type, _validate=False
        )

    def create_race_distribution(
        self,
        odds_input: Union[OddsStructure, Dict],
        **kwargs,
    ) -> RaceDistribution:
        """
        Create a complete race distribution from odds.

        Parameters
        ----------
        odds_input : Union[OddsStructure, Dict]
            OddsStructure instance or raw odds dictionary
        **kwargs
            Additional parameters for distribution creation including normalization:
            - method : str, optional
                Method to use for normalization, by default "sinkhorn"

        Returns
        -------
        RaceDistribution
            Race distribution containing all sessions
        """
        # Ensure input is an OddsStructure
        odds_structure = self._ensure_odds_structure(odds_input)

        # Create race session distribution
        race_dist = self.create_session_distribution(
            odds_input, session_type="race", **kwargs
        )

        # Create qualifying and sprint distributions if available in odds structure
        quali_dist = None
        if "qualifying" in odds_structure.sessions:
            quali_dist = self.create_session_distribution(
                odds_input, session_type="qualifying", **kwargs
            )

        sprint_dist = None
        if "sprint" in odds_structure.sessions:
            sprint_dist = self.create_session_distribution(
                odds_input, session_type="sprint", **kwargs
            )

        # Create and return RaceDistribution - it will handle missing sessions
        return RaceDistribution(
            race=race_dist, qualifying=quali_dist, sprint=sprint_dist
        )

    def _harville_fallback(
        self, win_probs: np.ndarray, driver_ids: list[str]
    ) -> Dict[str, PositionDistribution]:
        """
        Fall back to Harville method for win-only probabilities.

        This implementation calculates the exact probabilities for each driver finishing
        in each position according to the Harville model using dynamic programming.

        Parameters
        ----------
        win_probs : np.ndarray
            Array of win probabilities
        driver_ids : list[str]
            List of driver IDs

        Returns
        -------
        Dict[str, PositionDistribution]
            Dictionary mapping driver IDs to position distributions
        """
        n = len(win_probs)
        if n == 0:
            return {}

        # Initialize result grid
        grid = {driver_id: {} for driver_id in driver_ids}

        # For a single driver case
        if n == 1:
            grid[driver_ids[0]] = {1: 1.0}
            return {driver_ids[0]: PositionDistribution(grid[driver_ids[0]])}

        # ---------- Dynamic Programming Approach ----------
        # We use a "state" to represent which drivers are still available.
        # Each driver is represented by a bit in the state:
        # - bit is 1 if driver is available
        # - bit is 0 if driver is already assigned a position

        # Initialize DP table with zeros
        dp = np.zeros(1 << n)

        # Start with all drivers available (all bits set to 1)
        full_mask = (1 << n) - 1  # e.g., for n=3: 2^3 - 1 = 7 = 111 binary
        dp[full_mask] = 1.0  # Probability of starting state is 1.0

        # Process all states from all drivers available down to none
        for state in range(full_mask, -1, -1):
            # Skip states with zero probability
            if dp[state] == 0:
                continue

            # Determine which drivers are still available in this state
            available_drivers = self._get_available_drivers(state, n)

            # Skip if no drivers are available (shouldn't happen, but for safety)
            if not available_drivers:
                continue

            # Current position to assign (1-indexed)
            current_position = n - len(available_drivers) + 1

            # Calculate sum of win probabilities for available drivers
            total_available_prob = sum(win_probs[i] for i in available_drivers)

            # Process each available driver
            self._process_available_drivers(
                state,
                available_drivers,
                current_position,
                win_probs,
                driver_ids,
                total_available_prob,
                dp,
                grid,
            )

        # Convert grid to PositionDistribution objects
        return {
            driver_id: PositionDistribution(positions)
            for driver_id, positions in grid.items()
        }

    def _get_available_drivers(self, state: int, n: int) -> list[int]:
        """
        Get indices of available drivers from a state bit mask.

        Parameters
        ----------
        state : int
            Bit mask representing available drivers
        n : int
            Total number of drivers

        Returns
        -------
        list[int]
            Indices of available drivers
        """
        available_drivers = []
        for i in range(n):
            if state & (1 << i):  # Check if i-th bit is set
                available_drivers.append(i)
        return available_drivers

    def _process_available_drivers(
        self,
        state: int,
        available_drivers: list[int],
        current_position: int,
        win_probs: np.ndarray,
        driver_ids: list[str],
        total_available_prob: float,
        dp: np.ndarray,
        grid: Dict[str, Dict[int, float]],
    ) -> None:
        """
        Process available drivers for the current state and position.

        Parameters
        ----------
        state : int
            Current state bit mask
        available_drivers : list[int]
            Indices of available drivers
        current_position : int
            Current position to assign
        win_probs : np.ndarray
            Array of win probabilities
        driver_ids : list[str]
            List of driver IDs
        total_available_prob : float
            Sum of win probabilities for available drivers
        dp : np.ndarray
            Dynamic programming table
        grid : Dict[str, Dict[int, float]]
            Position probability grid to update
        """
        # Assign probabilities for the current position
        for driver_idx in available_drivers:
            # Skip if driver has zero probability
            if win_probs[driver_idx] <= 0:
                continue

            # Harville model: probability proportional to win probability
            conditional_prob = win_probs[driver_idx] / (total_available_prob + 1e-10)

            # Overall probability: current state probability Ã— conditional probability
            position_prob = dp[state] * conditional_prob

            # Update the grid for this driver and position
            driver_id = driver_ids[driver_idx]
            grid[driver_id][current_position] = (
                grid[driver_id].get(current_position, 0.0) + position_prob
            )

            # Compute next state by removing this driver
            next_state = state & ~(1 << driver_idx)

            # Add probability to next state in DP table
            dp[next_state] += position_prob

    def _convert_cumulative_to_position_distributions(
        self, cumulative_probs: Dict[int, Dict[str, float]], driver_ids: List[str]
    ) -> Dict[str, PositionDistribution]:
        """
        Convert cumulative market probabilities to position distributions.

        Parameters
        ----------
        cumulative_probs : Dict[int, Dict[str, float]]
            Nested dictionary with format:
            {
                1: {"VER": 0.4, "HAM": 0.3, ...},  # Win probabilities
                3: {"VER": 0.7, "HAM": 0.6, ...},  # Top-3 probabilities
                6: {"VER": 0.9, "HAM": 0.8, ...},  # Top-6 probabilities
                ...
            }
        driver_ids : List[str]
            List of all driver IDs to include in the distributions

        Returns
        -------
        Dict[str, PositionDistribution]
            Dictionary mapping driver IDs to position distributions
        """
        # Dictionary to store resulting distributions
        distributions = {}

        # Process each driver
        for driver_id in driver_ids:
            # Get cumulative probabilities for this driver
            driver_cum_probs, driver_quality = self._extract_driver_probs(
                driver_id, cumulative_probs
            )

            # Convert to position-specific probabilities
            position_probs = self._convert_driver_probs(
                driver_cum_probs, driver_quality
            )

            # Create position distribution
            try:
                distributions[driver_id] = PositionDistribution(position_probs)
            except Exception as e:
                # If validation fails, try to normalize and retry
                warnings.warn(
                    f"Initial distribution invalid for {driver_id}: {e}. "
                    f"Attempting normalization."
                )
                normalized = self._normalize_probs(position_probs)
                distributions[driver_id] = PositionDistribution(
                    normalized, _validate=False
                ).normalize()

        # Ensure column constraints
        distributions = self._ensure_column_constraints(distributions)

        return distributions

    def _extract_driver_probs(
        self,
        driver_id: str,
        cumulative_probs: Dict[int, Dict[str, float]],
        raise_on_missing: bool = False,
    ) -> tuple[Dict[int, float], float]:
        """
        Extract cumulative probabilities for a driver with validation.

        Parameters
        ----------
        driver_id : str
            Driver ID
        cumulative_probs : Dict[int, Dict[str, float]]
            Nested dictionary of cumulative probabilities
        raise_on_missing : bool, optional
            Whether to raise an exception on missing probabilities, by default False

        Returns
        -------
        tuple[Dict[int, float], float]
            Dictionary mapping thresholds to probabilities and driver quality estimate

        Raises
        ------
        ValueError
            If driver has no probabilities and raise_on_missing is True
        """
        thresholds = sorted(cumulative_probs.keys())
        driver_probs = {}

        # Extract available probabilities
        for threshold in thresholds:
            if driver_id in cumulative_probs.get(threshold, {}):
                driver_probs[threshold] = cumulative_probs[threshold][driver_id]

        # Ensure we have at least one probability
        if not driver_probs:
            error_msg = (
                f"No probabilities found for driver {driver_id}. "
                f"Available thresholds: {thresholds}. "
                f"Check if driver ID is correct or if odds data is complete."
            )

            if raise_on_missing:
                raise ValueError(error_msg)

            # If not raising, set default probabilities with warning
            default_win_prob = 0.01  # Small default win probability
            driver_probs[1] = default_win_prob
            driver_probs[self.max_position] = 1.0

            warnings.warn(
                f"{error_msg} Using defaults: win={default_win_prob}, "
                f"max={self.max_position}=1.0"
            )

        # Estimate driver quality based on win probability or early cumulative markets
        driver_quality = 0.5  # Default to mid-tier
        if 1 in driver_probs:
            driver_quality = driver_probs[
                1
            ]  # Win probability is best quality indicator
        elif 3 in driver_probs:
            driver_quality = driver_probs[3] / 3  # Estimate from top-3 probability
        elif thresholds:
            # Use earliest available threshold
            first_threshold = min(driver_probs.keys())
            driver_quality = driver_probs[first_threshold] / first_threshold

        # Add implicit threshold for full grid if not present
        if self.max_position not in driver_probs:
            driver_probs[self.max_position] = 1.0

        # Ensure monotonically increasing probabilities
        prev_threshold = 0
        prev_prob = 0.0
        ordered_probs = {}

        for threshold in sorted(driver_probs.keys()):
            prob = max(driver_probs[threshold], prev_prob)
            ordered_probs[threshold] = prob
            prev_threshold = threshold
            prev_prob = prob

        return ordered_probs, driver_quality

    def _convert_driver_probs(
        self,
        driver_cum_probs: Dict[int, float],
        driver_quality: float,
        strict_monotonicity: bool = False,
    ) -> Dict[int, float]:
        """
        Convert cumulative probabilities to position-specific probabilities.

        Parameters
        ----------
        driver_cum_probs : Dict[int, float]
            Dictionary mapping thresholds to cumulative probabilities
        driver_quality : float
            Measure of driver quality (0 to 1)
        strict_monotonicity : bool, optional
            Whether to raise errors on non-increasing probabilities, by default False

        Returns
        -------
        Dict[int, float]
            Dictionary mapping positions to probabilities

        Raises
        ------
        ValueError
            If strict_monotonicity is True and non-increasing probabilities are detected
        """
        # Sort thresholds
        sorted_thresholds = sorted(driver_cum_probs.keys())
        position_probs = {}

        # Process each segment defined by thresholds
        prev_threshold = 0
        prev_cum_prob = 0.0
        monotonicity_issues = []

        for threshold in sorted_thresholds:
            cum_prob = driver_cum_probs[threshold]

            # Probability mass for this segment
            segment_prob = cum_prob - prev_cum_prob

            # Handle non-increasing probabilities
            if segment_prob <= 0:
                issue = (
                    f"Non-increasing probabilities detected for thresholds "
                    f"{prev_threshold} to {threshold}. "
                    f"Values: {prev_cum_prob} -> {cum_prob}. "
                    f"This may indicate inconsistent market data."
                )
                monotonicity_issues.append(issue)

                if strict_monotonicity:
                    raise ValueError(issue)

                warnings.warn(issue + " Skipping segment.")
                prev_threshold = threshold
                prev_cum_prob = cum_prob
                continue

            # Define segment range
            start_pos = prev_threshold + 1
            end_pos = threshold

            # Get baseline weights for this segment
            if threshold != self.max_position or prev_threshold == 0:
                baseline_weights = self._get_baseline_weights(
                    start_pos, end_pos, prev_cum_prob, driver_cum_probs
                )
            else:
                # Special case for the tail segment
                baseline_weights = self._get_tail_weights(
                    start_pos, end_pos, driver_quality
                )

            # Scale weights to match segment probability
            total_weight = sum(baseline_weights.values())
            if total_weight > 0:
                for pos in range(start_pos, end_pos + 1):
                    if pos in baseline_weights:
                        position_probs[pos] = (
                            segment_prob * baseline_weights[pos] / total_weight
                        )

            prev_threshold = threshold
            prev_cum_prob = cum_prob

        # Ensure all positions have a probability
        for pos in range(1, self.max_position + 1):
            if pos not in position_probs:
                position_probs[pos] = 1e-10  # Very small but non-zero probability

        # Report monotonicity issues if any
        if monotonicity_issues and not strict_monotonicity:
            warnings.warn(
                f"Detected {len(monotonicity_issues)} monotonicity issues in "
                f"cumulative probabilities. This may affect distribution quality."
            )

        return position_probs

    def _get_baseline_weights(
        self,
        start_pos: int,
        end_pos: int,
        prev_cum_prob: float,
        driver_cum_probs: Optional[Dict[int, float]] = None,
    ) -> Dict[int, float]:
        """
        Get baseline weights for positions within a segment.

        Different methods can be used to assign weights:
        - "exponential": Exponential decay from start to end positions
        - "linear": Linear decay from start to end
        - "uniform": Equal weight for all positions

        Parameters
        ----------
        start_pos : int
            Starting position of the segment
        end_pos : int
            Ending position of the segment
        prev_cum_prob : float
            Previous cumulative probability (used for quality estimation)
        driver_cum_probs : Optional[Dict[int, float]], optional
            Complete driver cumulative probabilities for richer quality estimation

        Returns
        -------
        Dict[int, float]
            Dictionary mapping positions to baseline weights
        """
        weights = {}

        if start_pos > end_pos:
            return weights

        # Estimate driver quality based on win probability and cumulative probabilities
        driver_quality = prev_cum_prob
        if driver_cum_probs and 1 in driver_cum_probs:
            # Win probability is a better indicator of quality
            driver_quality = max(driver_quality, driver_cum_probs[1])

        if self.baseline_method == "exponential":
            # Get decay parameter (higher values = faster decay)
            # Better drivers (higher quality) should have faster decay
            base_decay = self.baseline_params.get("decay", 0.5)
            quality_factor = 1.0 + driver_quality  # Adjust decay by quality
            decay = base_decay * quality_factor

            # Create exponentially decaying weights
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = np.exp(-decay * (pos - start_pos))

        elif self.baseline_method == "linear":
            # Linear decay from start to end
            # Adjust steepness based on driver quality
            quality_factor = 1.0 + driver_quality
            for pos in range(start_pos, end_pos + 1):
                base_weight = end_pos + 1 - pos
                weights[pos] = base_weight * quality_factor

        elif self.baseline_method == "uniform":
            # Equal weights
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = 1.0

        else:
            # Default to exponential
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = np.exp(-0.5 * (pos - start_pos))

        # Special case for first (winning) position - adjust weight based on quality
        if start_pos == 1 and 1 in weights:
            # Driver "quality" approximated by previous cumulative probability
            quality_factor = 1.0 + (driver_quality * 2)  # Higher for favorites
            weights[1] *= quality_factor

        return weights

    def _get_tail_weights(
        self,
        start_pos: int,  # Position after last threshold
        end_pos: int,  # Maximum grid position (e.g., 20)
        driver_quality: float,  # Quality measure (e.g., win probability)
    ) -> Dict[int, float]:
        """
        Get weights for positions beyond the last threshold.

        Parameters
        ----------
        start_pos : int
            Starting position (after the last threshold)
        end_pos : int
            Maximum grid position
        driver_quality : float
            Measure of driver quality (0 to 1)

        Returns
        -------
        Dict[int, float]
            Weights for each position in the tail
        """
        weights = {}
        num_positions = end_pos - start_pos + 1

        if num_positions <= 0:
            return weights

        # Different patterns based on driver quality
        if driver_quality > 0.6:  # Strong driver
            # Decreasing exponential: higher prob at start_pos, lower at end_pos
            decay = 2.0  # Steeper decay for stronger drivers
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = np.exp(-decay * (pos - start_pos) / num_positions)
        elif driver_quality < 0.3:  # Weak driver
            # Increasing linear: lower prob at start_pos, higher at end_pos
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = 1.0 + (pos - start_pos) / (num_positions / 2)
        else:  # Mid-tier driver
            # Slightly decreasing: fairly flat but bit higher at start_pos
            for pos in range(start_pos, end_pos + 1):
                weights[pos] = 1.0 - 0.5 * (pos - start_pos) / num_positions

        return weights

    def _normalize_probs(self, probs: Dict[int, float]) -> Dict[int, float]:
        """
        Normalize probabilities to sum to 1.0.

        Parameters
        ----------
        probs : Dict[int, float]
            Dictionary mapping positions to probabilities

        Returns
        -------
        Dict[int, float]
            Normalized probabilities
        """
        total = sum(probs.values())
        if total <= 0:
            # If no valid probabilities, create uniform distribution
            return {i: 1.0 / self.max_position for i in range(1, self.max_position + 1)}
        return {k: v / total for k, v in probs.items()}

    def _ensure_column_constraints(
        self,
        distributions: Dict[str, PositionDistribution],
        tolerance: float = 1e-5,
        max_iterations: int = 100,
    ) -> Dict[str, PositionDistribution]:
        """
        Ensure each position has probability sum = 1.0 across all drivers.

        This step ensures the joint distribution is fully consistent across
        both rows (drivers) and columns (positions), while preserving the
        original win probabilities.

        Parameters
        ----------
        distributions : Dict[str, PositionDistribution]
            Dictionary mapping driver IDs to position distributions
        tolerance : float, optional
            Convergence tolerance threshold, by default 1e-5
        max_iterations : int, optional
            Maximum number of iterations, by default 100

        Returns
        -------
        Dict[str, PositionDistribution]
            Adjusted distributions
        """
        drivers = list(distributions.keys())
        n_drivers = len(drivers)
        n_positions = self.max_position

        if n_drivers == 0:
            return distributions

        # Create probability matrix [driver, position]
        P = np.zeros((n_drivers, n_positions))
        for i, driver in enumerate(drivers):
            for pos in range(1, n_positions + 1):
                P[i, pos - 1] = distributions[driver].get(pos)

        # Save original win probabilities
        original_win_probs = P[:, 0].copy()

        # Check if win probabilities sum to 1.0
        win_sum = original_win_probs.sum()
        if not np.isclose(win_sum, 1.0) and win_sum > 0:
            # Scale win probabilities to sum to 1.0
            original_win_probs = original_win_probs / win_sum

        # Iterative proportional fitting with convergence tracking
        iteration = 0
        max_change = float("inf")

        while max_change > tolerance and iteration < max_iterations:
            P_old = P.copy()

            # Row normalization
            row_sums = P.sum(axis=1, keepdims=True)
            P = P / np.where(row_sums > 0, row_sums, 1.0)

            # Restore original win probabilities
            P[:, 0] = original_win_probs

            # Column normalization (except position 1)
            for pos in range(1, n_positions):
                col_sum = P[:, pos].sum()
                if col_sum > 0:
                    P[:, pos] = P[:, pos] / col_sum

            # Re-normalize each driver's remaining probabilities to ensure row sums to 1
            for i in range(n_drivers):
                win_prob = P[i, 0]
                remaining_prob = 1.0 - win_prob
                row_sum = P[i, 1:].sum()
                if row_sum > 0:
                    P[i, 1:] = P[i, 1:] * (remaining_prob / row_sum)

            # Calculate maximum change
            max_change = np.max(np.abs(P - P_old))
            iteration += 1

        # Log convergence info if available
        if hasattr(self, "logger"):
            if max_change <= tolerance:
                self.logger.debug(f"Convergence reached after {iteration} iterations")
            else:
                self.logger.warning(
                    f"Failed to converge after {max_iterations} iterations. "
                    f"Final max change: {max_change:.6f}"
                )

        # Convert back to PositionDistribution objects
        result = {}
        for i, driver in enumerate(drivers):
            position_probs = {
                pos + 1: P[i, pos] for pos in range(n_positions) if P[i, pos] > 0
            }
            result[driver] = PositionDistribution(
                position_probs, _validate=False
            ).normalize()

        return result
